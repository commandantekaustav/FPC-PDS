{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 9: Exploring Unstructured Data (Text)\n",
    "\n",
    "**Unit 1: Introduction to Data Science**\n",
    "**Hour: 9**\n",
    "**Mode: Practical Lab**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "This lab introduces the basics of handling unstructured data, focusing on plain text. We will perform the most fundamental task in text analysis: reading text and counting word frequencies.\n",
    "\n",
    "**What is Unstructured Data?** Data that has no inherent model or organization, like the text in a book or an email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "We will use a special tool from Python's `collections` library called `Counter`, which is perfect for counting items in a list. We will also use the `re` library for regular expressions to help us clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Text Data\n",
    "\n",
    "Let's create a simple text file to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"Data science is a field of study. \n",
    "This field combines domain expertise, programming skills, and knowledge of mathematics and statistics. \n",
    "Data scientists use this field to extract knowledge and insights from data.\"\"\"\n",
    "\n",
    "with open('sample.txt', 'w') as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Basic Text Processing Workflow\n",
    "\n",
    "We will follow a simple multi-step process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Step 1: Read the File\n",
    "\n",
    "We use Python's built-in file handling to open the file and read its contents into a single string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Step 2: Normalize and Clean\n",
    "\n",
    "To ensure that \"Data\" and \"data\" are treated as the same word, we convert the entire string to lowercase. We will also remove punctuation using a regular expression. `re.sub(r'[^\\w\\s]', '', text)` finds anything that is NOT a word character (`\\w`) or whitespace (`\\s`) and replaces it with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "print(\"Cleaned Text:\")\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Step 3: Tokenization (Splitting into Words)\n",
    "\n",
    "**Tokenization** is breaking text into smaller pieces, or \"tokens\". We'll split the cleaned string by spaces to get a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text_clean.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Step 4: Count Word Frequencies\n",
    "\n",
    "Now we use the `Counter` object to count the occurrences of each unique word in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object has a helpful method called `.most_common()` to see the top N words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 5 most common words\n",
    "print(word_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "While this was a simple example, you have just performed the foundational steps of almost any text analysis project:\n",
    "1.  Read raw text data.\n",
    "2.  Normalize and clean the text (lowercase, remove punctuation).\n",
    "3.  Tokenize the text into words.\n",
    "4.  Calculate frequencies to find the most important terms.\n",
    "\n",
    "This process is the starting point for more advanced topics like sentiment analysis, topic modeling, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}