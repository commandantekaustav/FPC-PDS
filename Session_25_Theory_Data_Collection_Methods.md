# Session 25: Data Collection Methods

**Unit 3: Data Collection and Cleaning**
**Hour: 25**
**Mode: Theory**

---

### 1. Objective

By the end of this session, students will be able to:
*   Identify and describe the four primary methods of data collection for data science projects.
*   Understand the context in which each method is most appropriate.
*   Recognize the potential challenges associated with each method.

---

### 2. Lecture & Discussion Points

Data doesn't magically appear; we have to **obtain** it. The method we use depends on the problem we're trying to solve and the resources available.

#### Method 1: Using Existing Databases

*   **Description:** This is the most common scenario in a corporate environment. The data already exists within the company's systems, stored in a structured database.
*   **How it works:** Data scientists use a query language, almost always **SQL (Structured Query Language)**, to extract the specific data they need from the database.
    *   `SELECT` specific columns.
    *   `FROM` a specific table.
    *   `WHERE` certain conditions are met.
*   **Analogy:** You need information about your customers. Instead of asking them one by one, you ask the company's highly organized librarian (the database) to pull the exact records you need.
*   **When to use it:** When the data you need is generated by your own company's operations (e.g., sales, customers, inventory).
*   **Challenges:**
    *   Requires knowledge of SQL.
    *   May need permission to access sensitive data.
    *   The data might be spread across many different tables that need to be "joined" together.

#### Method 2: Application Programming Interfaces (APIs)

*   **Description:** An API is a structured way for computer programs to talk to each other. Many web services (like Twitter, Google Maps, weather services) provide APIs that allow you to request their data in a clean, predictable format (usually JSON).
*   **How it works:** You write a script (e.g., in Python) that sends a request to the API's URL, often with a special key to identify yourself. The API sends back the data you asked for.
*   **Analogy:** You go to a restaurant (the web service). Instead of going into the kitchen yourself (accessing their database directly), you give your order to a waiter (the API). The waiter brings you exactly what you ordered in a nicely presented way.
*   **When to use it:** When you need live, up-to-date data from an external service.
*   **Challenges:**
    *   Often have **rate limits** (you can only make a certain number of requests per hour/day).
    *   May require payment or an authentication key.
    *   You are limited to the data the service chooses to expose through its API.

#### Method 3: Web Scraping

*   **Description:** The process of automatically extracting information from unstructured HTML web pages. This is used when a website has data you want but does not provide an API.
*   **How it works:** You write a script that:
    1.  Downloads the raw HTML content of a web page.
    2.  Parses the HTML to navigate its structure.
    3.  Extracts the specific text or numbers you need.
*   **Analogy:** If a website is a newspaper, web scraping is like using a pair of scissors and a ruler to methodically cut out every headline and paste it into a scrapbook.
*   **When to use it:** As a last resort, when the data is publicly visible on a website but not available via any other means.
*   **Challenges:**
    *   **Fragile:** If the website changes its layout, your scraper will break.
    *   **Ethical & Legal Gray Area:** You must check a website's `robots.txt` file and terms of service. Scraping too aggressively can get your IP address blocked.
    *   **Complex:** Websites with complex JavaScript can be very difficult to scrape.

#### Method 4: Manual Data Collection (Surveys)

*   **Description:** Designing and conducting surveys, experiments, or observations to create a brand new dataset from scratch.
*   **How it works:** Use tools like Google Forms, SurveyMonkey, or conduct formal scientific experiments.
*   **Analogy:** You are a biologist who wants to study a new species. There are no books or records about it, so you must go into the field with a notebook and record your own observations.
*   **When to use it:** When the data you need simply does not exist anywhere else. Often used in academic research, user experience studies, and market research.
*   **Challenges:**
    *   **Time-consuming and expensive.**
    *   **Prone to bias:** The way you word questions or select participants can heavily influence the results (e.g., selection bias, response bias).
    *   **Small scale:** It's difficult to collect a massive dataset this way.

---

### 3. In-Class Activity

**Discussion Prompt:** "For our TelCo churn project, which data collection method was used?"

*   *Answer:* **Method 1: Using Existing Databases.** The data (customer profiles, usage, etc.) is all internal data that would be stored in the company's own systems. A data scientist at TelCo would have written a SQL query to extract this information and save it as the CSV file we are using.

**Prompt 2:** "If you wanted to analyze the sentiment of tweets about your company, which method would you use?"

*   *Answer:* **Method 2: APIs.** Twitter provides a public API specifically for this purpose.

**Prompt 3:** "If you wanted to get the price of every product from a competitor's e-commerce website that does not have an API, which method would you use?"

*   *Answer:* **Method 3: Web Scraping.**

---

### 4. Session Wrap-up

*   No single data collection method is best; the right choice depends entirely on the project's needs.
*   In a corporate setting, querying internal databases via SQL is the most common starting point.
*   APIs are the preferred way to get data from external services, while web scraping is a powerful but more fragile alternative.
*   **Next Session:** Once we have the data, it's rarely perfect. We will discuss the core concepts of data cleaning.