{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 27: Web Scraping with Requests and BeautifulSoup (Part 1)\n",
    "\n",
    "**Unit 3: Data Collection and Cleaning**\n",
    "**Hour: 27**\n",
    "**Mode: Practical Lab**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "This is our first hands-on lab for data collection. We will learn the two-step process for scraping a simple website:\n",
    "1.  Use the `requests` library to download the HTML content of a web page.\n",
    "2.  Use the `BeautifulSoup` library to parse the HTML and make it searchable.\n",
    "\n",
    "We will be scraping quotes from [quotes.toscrape.com](http://quotes.toscrape.com/), a website designed specifically for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "We need to import the two key libraries for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Step 1: Making the Request\n",
    "\n",
    "We use the `requests.get()` function to send an HTTP GET request to the website's URL. This is like what your browser does when you type in a web address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object contains the server's response. We can check the `status_code` to see if our request was successful.\n",
    "\n",
    "*   `200` means OK.\n",
    "*   `404` means Not Found.\n",
    "*   `500` means Server Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw HTML content is stored in the `.text` attribute of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 500 characters of the HTML\n",
    "html_content = response.text\n",
    "print(html_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is messy and hard to read. We need a parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Step 2: Parsing with BeautifulSoup\n",
    "\n",
    "We create a `BeautifulSoup` object by passing it our `html_content` and telling it which parser to use (`'html.parser'` is a good built-in choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `soup` object is now a structured representation of the website. We can use its methods to find specific HTML elements.\n",
    "\n",
    "For example, let's find the `<title>` tag of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = soup.title.text # .text extracts only the text content\n",
    "print(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Finding a Single Element\n",
    "\n",
    "To find elements, we need to inspect the web page's source code (in your browser, right-click -> \"Inspect\").\n",
    "\n",
    "After inspecting [quotes.toscrape.com](http://quotes.toscrape.com/), we see that each quote is contained within a `<div>` that has the class `quote`.\n",
    "\n",
    "Let's find the **first** quote on the page using the `.find()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first div with class='quote'\n",
    "first_quote_div = soup.find('div', class_='quote')\n",
    "\n",
    "print(first_quote_div.prettify()) # .prettify() formats the HTML nicely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've isolated the quote's container, we can search *within* it to get the text and the author.\n",
    "\n",
    "The quote text is in a `<span>` with `class='text'`. The author is in a `<small>` with `class='author'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_text = first_quote_div.find('span', class_='text').text\n",
    "author_name = first_quote_div.find('small', class_='author').text\n",
    "\n",
    "print(f\"Quote: {quote_text}\")\n",
    "print(f\"Author: {author_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "In this lab, you have learned the fundamental two-step process of web scraping:\n",
    "1.  Using `requests` to download the HTML content of a page.\n",
    "2.  Using `BeautifulSoup` to parse the HTML.\n",
    "3.  Using the `.find()` method to locate the first occurrence of a specific HTML element based on its tag and class.\n",
    "4.  Extracting the `.text` from a found element.\n",
    "\n",
    "**Next Session:** We will build on this by learning how to find *all* matching elements on a page and how to organize the scraped data into a structured Pandas DataFrame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}