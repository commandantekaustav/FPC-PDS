{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 31: Data Cleaning Part 3 (Handling Duplicates)\n",
    "\n",
    "**Unit 3: Data Collection and Cleaning**\n",
    "**Hour: 31**\n",
    "**Mode: Practical Lab**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "This lab focuses on another common data cleaning task: identifying and removing duplicate records from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "Let's start with a fresh, simple DataFrame to make the concept of duplicates clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'id': [1, 2, 3, 2], # Note the duplicate id 2\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Bob'] # Note the duplicate name Bob\n",
    "}\n",
    "\n",
    "df_simple = pd.DataFrame(data)\n",
    "df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the last row (index 3) is an exact duplicate of the row at index 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identifying Duplicates\n",
    "\n",
    "Pandas provides the `.duplicated()` method, which returns a boolean Series indicating whether a row is a duplicate of a *previous* row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first occurrence of the Bob/2 row (at index 1) is marked `False`, while the second occurrence (at index 3) is marked `True`.\n",
    "\n",
    "We can use `.sum()` to quickly count the number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicate rows: {df_simple.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Removing Duplicates\n",
    "\n",
    "The `.drop_duplicates()` method finds and removes duplicate rows, keeping the first occurrence by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_simple.drop_duplicates()\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicate row has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Application to the Telco Dataset\n",
    "\n",
    "Now, let's apply this to our main Telco dataset. Does it have any duplicate rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data again\n",
    "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in the Telco dataset: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding:** The Telco dataset is already clean and has no duplicate rows. This is great, but not always the case in real-world projects.\n",
    "\n",
    "Sometimes, you might consider a duplicate based on a subset of columns. For example, is any `customerID` repeated? A customer ID should be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_duplicates = df.duplicated(subset=['customerID']).sum()\n",
    "print(f\"Number of duplicate customer IDs: {customer_id_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 0, confirming that every customer has a unique ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "In this session, you learned the straightforward process for handling duplicate data:\n",
    "1.  Use `.duplicated().sum()` to quickly check if and how many duplicate rows exist.\n",
    "2.  Use `.drop_duplicates()` to remove them.\n",
    "3.  Use the `subset` parameter in these methods to check for duplicates based on specific key columns (like an ID).\n",
    "\n",
    "While our main dataset was clean, this is a crucial check in any data cleaning workflow.\n",
    "\n",
    "**Next Session:** We will explore how to identify and handle outliers in our numerical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}