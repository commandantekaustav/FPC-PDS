{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 36: Tying It All Together (Intro to EDA)\n",
    "\n",
    "**Unit 3: Data Collection and Cleaning**\n",
    "**Hour: 36**\n",
    "**Mode: Practical Lab**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "This lab serves as a capstone for Unit 3. We will perform a streamlined, end-to-end cleaning and preparation workflow on the Telco dataset, combining all the techniques we've learned so far. The output will be a clean dataset, ready for the deep-dive analysis and visualization we will perform in Unit 4.\n",
    "\n",
    "This is the bridge between the **Scrub** and **Explore** phases of the OSEMN workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "Import Pandas and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Cleaning and Preparation Workflow\n",
    "\n",
    "Let's execute our cleaning plan step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Fix `TotalCharges` Data Type and Handle Missing Values\n",
    "\n",
    "We'll combine the steps from Sessions 29 and 30 into a single, efficient process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce to numeric, creating NaNs for blank strings\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Calculate the median\n",
    "median_charges = df['TotalCharges'].median()\n",
    "\n",
    "# Fill the missing values with the median\n",
    "df['TotalCharges'].fillna(median_charges, inplace=True)\n",
    "\n",
    "# Verify the fix\n",
    "print(\"Data type of TotalCharges after fix:\")\n",
    "print(df['TotalCharges'].dtype)\n",
    "print(\"\\nNumber of missing values in TotalCharges after fix:\")\n",
    "print(df['TotalCharges'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Check for Duplicates\n",
    "\n",
    "As we discovered in Session 31, there are no full-row duplicates, but this is a crucial check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicate rows found: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Standardize Categorical Values (Example)\n",
    "\n",
    "Let's look at the values in the `PaymentMethod` column. They seem consistent, but in a real-world scenario, you might have variations.\n",
    "\n",
    "**Hypothetical Problem:** Imagine we also had `\"Credit Card (automatic)\"` with a different capitalization. We would need to standardize it.\n",
    "\n",
    "While not strictly necessary here, this is how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of standardization using .replace()\n",
    "# This won't change anything in our current dataset, but it demonstrates the technique\n",
    "df['PaymentMethod'] = df['PaymentMethod'].replace({\n",
    "    'Credit card (automatic)': 'Credit Card (automatic)',\n",
    "    'Bank transfer (automatic)': 'Bank Transfer (automatic)'\n",
    "})\n",
    "\n",
    "df['PaymentMethod'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create a New Feature\n",
    "\n",
    "Let's re-create our `TenureGroup` feature from Session 35, as it will be useful for our upcoming analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 12, 48, 73] # (0-12 months), (13-48 months), (49-72 months)\n",
    "labels = ['New Customer', 'Medium-Term Customer', 'Long-Term Customer']\n",
    "\n",
    "df['TenureGroup'] = pd.cut(df['tenure'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Review of the Cleaned Data\n",
    "\n",
    "Let's take one last look at our prepared dataset with `.info()` and `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now clean and ready for analysis!\n",
    "*   No missing values.\n",
    "*   Correct data types.\n",
    "*   A new, engineered feature (`TenureGroup`) is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "This session consolidated the key data cleaning and preparation tasks into a single, logical workflow. We have successfully transitioned our raw data into a clean, analyzable state.\n",
    "\n",
    "This prepared dataset will be the foundation for all the work we do in Unit 4, where we will dive deep into Exploratory Data Analysis (EDA) to uncover insights.\n",
    "\n",
    "**Next Session:** We will begin Unit 4 with a theoretical discussion of the key statistical concepts used to describe and summarize data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}