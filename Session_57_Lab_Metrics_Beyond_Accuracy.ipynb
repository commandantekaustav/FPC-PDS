{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 57: Classification Metrics Beyond Accuracy\n",
    "\n",
    "**Unit 5: Basics of Predictive Analytics**\n",
    "**Hour: 57**\n",
    "**Mode: Practical Lab**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Objective\n",
    "\n",
    "This lab dives deeper into classification evaluation, moving beyond accuracy. We will learn how to calculate and interpret **Precision**, **Recall**, and the **F1-Score** from the confusion matrix. These metrics give us a more nuanced understanding of our model's performance, especially on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "Let's recreate our Logistic Regression model, predictions, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load and prep data\n",
    "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(url)\n",
    "df_subset = df[['tenure', 'MonthlyCharges', 'Contract', 'Churn']].copy()\n",
    "df_subset.dropna(inplace=True)\n",
    "\n",
    "# Prep data and train model\n",
    "X = df_subset.drop('Churn', axis=1)\n",
    "y = df_subset['Churn']\n",
    "X_encoded = pd.get_dummies(X, columns=['Contract'], drop_first=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model and make predictions\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "# Get our TN, FP, FN, TP values from last session\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understanding the Metrics\n",
    "\n",
    "#### 3.1. Precision\n",
    "*   **Question it answers:** Of all the customers the model **predicted** would churn, what percentage actually did?\n",
    "*   **Formula:** `Precision = TP / (TP + FP)`\n",
    "*   **Business Relevance:** High precision means you have a low **False Positive Rate**. When your model predicts a customer will churn, it's very likely to be correct. This is important when the cost of acting on a prediction is high (e.g., you don't want to waste money giving discounts to happy customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate using the formula\n",
    "precision_manual = tp / (tp + fp)\n",
    "print(f\"Manual Precision: {precision_manual:.4f}\")\n",
    "\n",
    "# Calculate using scikit-learn\n",
    "# pos_label='Yes' tells the function that 'Yes' is our positive class\n",
    "precision_sklearn = precision_score(y_test, y_pred, pos_label='Yes')\n",
    "print(f\"Scikit-learn Precision: {precision_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** When our model predicts a customer will churn, it is correct about **63.6%** of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Recall (or Sensitivity)\n",
    "*   **Question it answers:** Of all the customers who **actually** churned, what percentage did our model correctly identify?\n",
    "*   **Formula:** `Recall = TP / (TP + FN)`\n",
    "*   **Business Relevance:** High recall means you have a low **False Negative Rate**. Your model is good at \"finding\" all the true churners. This is crucial when the cost of missing a case is very high (like missing a fraudulent transaction or a patient with a disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate using the formula\n",
    "recall_manual = tp / (tp + fn)\n",
    "print(f\"Manual Recall: {recall_manual:.4f}\")\n",
    "\n",
    "# Calculate using scikit-learn\n",
    "recall_sklearn = recall_score(y_test, y_pred, pos_label='Yes')\n",
    "print(f\"Scikit-learn Recall: {recall_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** Our model successfully identified only **51.7%** of all the customers who actually churned. This is a weakness; we are missing almost half of the churners (high False Negative rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. The Precision-Recall Trade-off\n",
    "\n",
    "Often, improving precision will lower recall, and vice-versa. A model that is very cautious will have high precision but might miss some cases (low recall). A model that tries to catch every case will have high recall but might have more false alarms (low precision). The best model finds a good balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. F1-Score\n",
    "*   **Question it answers:** What is the harmonic mean of Precision and Recall?\n",
    "*   **Formula:** `F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "*   **Business Relevance:** The F1-score is a single metric that combines both precision and recall. It's useful when you need to balance both concerns and find a model that is both accurate in its positive predictions and good at finding all the positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** The F1-score of 0.57 provides a balanced summary of our model's performance on the positive class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion\n",
    "\n",
    "In this lab, you went beyond accuracy to evaluate your classification model:\n",
    "1.  **Precision:** Measures the accuracy of positive predictions.\n",
    "2.  **Recall:** Measures the model's ability to find all actual positive cases.\n",
    "3.  **F1-Score:** Provides a single, balanced metric between Precision and Recall.\n",
    "\n",
    "For our churn model, we learned it has decent precision but poor recall (it misses too many churners). This tells us that if we wanted to improve this model, we should focus on techniques that reduce False Negatives.\n",
    "\n",
    "**Next Session:** We will learn how to interpret our model to understand *why* it is making the predictions it does."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}